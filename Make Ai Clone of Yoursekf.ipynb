{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"id":"CgxobX5hwzbh","executionInfo":{"status":"ok","timestamp":1745167324579,"user_tz":-330,"elapsed":26962,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3468f1f1-18b5-43d5-a6ae-e2a9f0dc91cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-vs_n4_dg/unsloth_e2e92cb0e98a42beb0cd7dc9afddfe0b\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-vs_n4_dg/unsloth_e2e92cb0e98a42beb0cd7dc9afddfe0b\n","  Resolved https://github.com/unslothai/unsloth.git to commit 6c234d5a66adb76b9b93fb0f2445648199d88e66\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: unsloth_zoo>=2025.3.17 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.17)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n","Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.19)\n","Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.51.3)\n","Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.2)\n","Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.30.2)\n","Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n","Requirement already satisfied: bitsandbytes>=0.43.3 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.5)\n","Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.15)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.5.3)\n","Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\n","Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.2)\n","Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.6)\n","Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.14.0)\n","Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.1.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.1.0)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.2)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.19.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.1.31)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n","Requirement already satisfied: xformers<0.0.27 in /usr/local/lib/python3.11/dist-packages (0.0.26.post1)\n","Requirement already satisfied: trl<0.9.0 in /usr/local/lib/python3.11/dist-packages (0.8.6)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n"]}],"source":["# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"]},{"cell_type":"markdown","metadata":{"id":"8VLYYB1Bwzbi"},"source":["### Background\n","\n","I saw a reel on Instagram in which an AI enthusiast created an AI clone of himself to talk to his girlfriend (Certainly, I won't do that... xd) using [RAG](https://youtu.be/YVWxbHJakgg?feature=shared) Retrieval Augmented Generation and the Chat GPT-3.5 turbo API. It kind of worked, but it had major privacy issues. Sending personal chats to Chat GPT could potentially result in those chats being used by OpenAI to train its model. This led me to think, what if I fine-tuned a pre-existing model like Llama or Mixtral on my personal WhatsApp chat history? It would be cool to have a model that can talk like me as I do on WhatsApp, primarily in Hinglish (Hindi + English).\n","\n","[Fine-tuning a large language model (LLM)](https://youtu.be/YVWxbHJakgg?feature=shared) requires a good understanding of LLMs in general. The major challenge with fine-tuning big models, such as a 7B parameter model, is that it requires a minimum of 32GB of GPU RAM, which is costly and not available in free-tier GPU compute services like Colab or Kaggle. So, I had to find a way around this limitation.\n","\n","Another important aspect is that the fine-tuning results heavily depend on the quality and size of the dataset used. Converting raw WhatsApp chat data into a usable dataset is challenging but worth pursuing.\n","\n","Let's see how it looks in reality and how it's being carried out."]},{"cell_type":"markdown","metadata":{"id":"IBoJ_Xw7wzbl"},"source":["### Important\n","\n","We are using a free instance of Google Colab to fine-tune our model`(Llama3)`, making it **totally free**.\n","\n","For chatting with our fine-tuned model, we will use [Ollama](https://ollama.com/) locally, which is very lightweight and requires only **8GB** of free RAM in your laptop/PC and works without any **GPU** support.\n","\n","**Keep in mind that your chat data is completely safe; it is not being sent to anyone.**"]},{"cell_type":"markdown","metadata":{"id":"vITh0KVJ10qX"},"source":["\n","### Data Prep\n","To extract chat history from your WhatsApp chats, follow these steps:\n","\n","1. Open your WhatsApp application.\n","2. Go to the chat from which you want to extract the chat history.\n","3. Click on the three dots in the top right corner of the screen.\n","4. Click on `More` then click on `Export Chat`.\n","5. Select `Without media`.\n","6. Save it locally or send it to saved messages on Telegram so you can later download it on your Telegram desktop.\n","#### **7. Repeat these steps for all of your chats. The more chat data you have, the better the results will be.**\n","\n","It will generate `.zip file`. You don't have to extract it.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TqvPGRZDwzbl"},"source":["#### Upload Exported Chat files to Colab runtime\n","Now, locate your exported chat zip files and upload them to the Colab runtime. Follow these steps to upload files to Google Colab:\n","\n","1. Click on the Files icon on the left side of the screen (as shown in the image attached below).\n","2. Click on the upload button. It will open the File Explorer. Choose the exported chat zip files\n","\n","##### **You can select multiple files at once**.\n","\n","* Wait until your files are uploaded. The upload process bar will display at the bottom left corner of the screen.\n","* Once files are uploaded successfully, they will appear in the Files tab of Google Colab.\n","\n","<img src=\"https://github.com/Eviltr0N/Make-AI-Clone-of-Yourself/raw/main/img/file_upload.png\" width=\"480\" height=\"600\">\n","\n","##### Keep in Mind:\n","\n","* Export chat history only for meaningful conversations. Before exporting a chat, consider whether it adds value to the data or if it is just a short conversation.\n","* If you think you don’t want the AI to learn from a specific chat, then don’t export it.\n","#### **Currently, it only supports individual chats, so please do not export group chats.**\n"]},{"cell_type":"markdown","metadata":{"id":"O5JN21BSwzbm"},"source":["### Data Filtering\n","\n","The exported data contains many irregularities such as `<Media omitted>`, `This message was deleted` and timestamps of messages. We need to remove these and convert the whole chat history into a `Prompt: Response` format so it can be used to fine-tune the model. To extract messages from the data, I used `regex`. Additionally, I filtered out any links and emails from the chat data for obvious privacy reasons.\n","\n","**Now, please edit the below list of `filler_words`**. These words may vary from person to person. Some examples are `Ok`, `Yup`, `Hmm`, `Han`. We need to remove these from the dataset because if we don't, the fine-tuned model will primarily learn these words and, in most cases, respond with them. For example, if you ask, \"Where are you going?\" the model might respond with something like \"Ok\" or \"Hmm.\""]},{"cell_type":"markdown","metadata":{"id":"44cyft5Jwzbm"},"source":["To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"zPGexEG7wzbm","executionInfo":{"status":"ok","timestamp":1745167347202,"user_tz":-330,"elapsed":62,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["filler_words = [\"Ok\", \"Okay\", \"Yup\", \"Hmm\"]\n","# Add or remove words from this list based on your personal usage.\n","\n","chat_dir = \"./\""]},{"cell_type":"code","execution_count":11,"metadata":{"id":"BdgsnIpGwzbn","executionInfo":{"status":"ok","timestamp":1745167349232,"user_tz":-330,"elapsed":5,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["import re\n","import os\n","import shutil\n","import csv"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-PvTvhPVwzbn","executionInfo":{"status":"ok","timestamp":1745167350835,"user_tz":-330,"elapsed":20,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["class Wh_Chat_Processor:\n","    def __init__(self):\n","        pass\n","    def open_chat_file(self, dir,filename):\n","        self.sender_name = filename.replace(\"WhatsApp Chat with \", \"\").replace(\".txt\", \"\")\n","        with open(os.path.join(dir,filename)) as f:\n","            chat_text = f.read()\n","        return chat_text\n","\n","    def msg_filter_basic(self, chat_text):\n","        filtered = []\n","        pt = r' - ([^:]+): (.*?)(?=\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2}\\s*(?:AM|PM|am|pm)? - |$)'\n","        msgs = re.findall(pt, chat_text, re.DOTALL)\n","        for msg in msgs:\n","            line = msg[1]\n","            wh_default_filter = \"Tap to learn more.\" in line or \"<Media omitted>\" in line\n","            website_filter = \"https://\" in line or \"http://\" in line\n","            mail_filter = \"@gmail.com\" in line\n","            deleted_msg_filter = \"This message was deleted\" in line or \"You deleted this message\" in line or \"<This message was edited>\" in line or \"(file attached)\" in line\n","\n","            if not (wh_default_filter or website_filter or mail_filter or deleted_msg_filter):\n","                    filtered.append(msg)\n","        return filtered\n","\n","    def process_chat(self, chat_data):\n","        merged_lines = []\n","        current_sender = None\n","        current_message = {}\n","        for line in chat_data:\n","            if not line:\n","                continue\n","            parts = line\n","            if len(parts) == 2:\n","                sender, message = parts\n","                if current_sender is None:\n","                    current_sender = sender\n","                    current_message[current_sender] = [message.strip()]\n","                elif sender == current_sender:\n","                    current_message[current_sender].append(message.strip())\n","                else:\n","                    merged_lines.append(current_message)\n","                    current_sender = sender\n","                    current_message = {current_sender: [message.strip()]}\n","            else:\n","                if current_sender:\n","                    current_message[current_sender][-1] += \" \" + line.strip()\n","        if current_sender:\n","            merged_lines.append(current_message)\n","        keys = set()\n","        for line in merged_lines:\n","            # print(line)\n","            for key in line.keys():\n","                if key != self.sender_name:\n","                    keys.add(key)\n","        self.my_name = list(keys)[0]\n","        print(list(keys))\n","        return merged_lines\n","\n","    def advance_filter(self, merged_chat_data):\n","        filtered_data=[]\n","        sender = \"\"\n","        me = \"\"\n","        chk = 1\n","        CD = merged_chat_data\n","        for ind, x in enumerate(CD):\n","            if x.get(self.sender_name) != None :\n","                if len(x[self.sender_name]) == 1 and ( x[self.sender_name][0] in filler_words or len(x[self.sender_name][0]) ==1 ):\n","                    continue\n","                if len(CD[ind][self.sender_name]) > 1:\n","                    for y in range(0,len(CD[ind][self.sender_name])):\n","                        if y+1 != len(CD[ind][self.sender_name]):\n","                            sender += CD[ind][self.sender_name][y] + \"\\n\"\n","                        else:\n","                            sender += CD[ind][self.sender_name][y]\n","                else:\n","                    sender += CD[ind][self.sender_name][0]\n","            elif x.get(self.my_name) != None and len(sender) > 1:\n","                if len(CD[ind][self.my_name]) > 1:\n","                    for y in range(0,len(CD[ind][self.my_name])):\n","                        if y+1 != len(CD[ind][self.my_name]):\n","                            me += CD[ind][self.my_name][y] + \"\\n\"\n","                        else:\n","                            me += CD[ind][self.my_name][y]\n","                else:\n","                    me += CD[ind][self.my_name][0]\n","            else:\n","                continue\n","            if chk ==1:\n","                chk+=1\n","            elif chk ==2:\n","                filtered_data.append([sender, me])\n","                sender = \"\"\n","                me=\"\"\n","                chk=1\n","            else:\n","                pass\n","        return filtered_data\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"0DVwbFW1wzbo","executionInfo":{"status":"ok","timestamp":1745167355536,"user_tz":-330,"elapsed":5,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["with open(\"all_chat_data.csv\", \"w\") as f:\n","    f.write(\"Prompt,Response\"+ \"\\n\")\n","\n","for file in os.listdir(os.path.join(chat_dir)):\n","    if file.endswith('.zip'):\n","        full_path = os.path.join(chat_dir, file)\n","        shutil.unpack_archive(full_path, chat_dir)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5f7ieid2wzbo","executionInfo":{"status":"ok","timestamp":1745167357218,"user_tz":-330,"elapsed":9,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b23ccfb2-874e-46b8-8abc-0e8cb9431fbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully Processed all the chats... Generated CSV File of chats is saved in Current directory with the name 'all_chat_data.csv'\n"]}],"source":["for file in os.listdir(os.path.join(chat_dir)):\n","    processor = Wh_Chat_Processor()\n","    if file.endswith('.txt'):\n","        print(\"Processing: \",file)\n","        chat_d = processor.open_chat_file(chat_dir,file)\n","        basic_f = processor.msg_filter_basic(chat_d)\n","        chat_ps = processor.process_chat(basic_f)\n","        filtered_data = processor.advance_filter(chat_ps)\n","        with open(\"all_chat_data.csv\", \"a\") as f:\n","            csv_writer = csv.writer(f)\n","            for row in filtered_data:\n","                csv_writer.writerow(row)\n","print(\"Successfully Processed all the chats... Generated CSV File of chats is saved in Current directory with the name 'all_chat_data.csv'\")"]},{"cell_type":"markdown","metadata":{"id":"c7FUCDIrwzbo"},"source":["### Model Fine-Tuning\n","As we discussed earlier, fine-tuning a 7B parameter model with just 16GB of RAM is not possible. To achieve this, we will use a technique known as [Quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization). Specifically, we will use 4-bit quantization.\n","\n","I am using [Unsloth](https://github.com/unslothai/unsloth) for the rest of the processes, such as quantization and training the model. Unsloth has very good documentation and requires less VRAM to fine-tune the model."]},{"cell_type":"markdown","metadata":{"id":"IqM-T1RTzY6C"},"source":["Check out - [Unsloth's Github](https://github.com/unslothai/unsloth)\n","\n","This notebook uses the `Llama-3` format for conversation style finetunes. We use [Open Assistant conversations](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) in ShareGPT style."]},{"cell_type":"markdown","metadata":{"id":"r2v_X2fA0Df5"},"source":["* Unsloth support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n","* Unsloth support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n"]},{"cell_type":"markdown","metadata":{"id":"iSvaRuHpwzbp"},"source":["For finetuning I am using **`Llama3` 8B Instruct** as our base model, you can use other models such as `Mixtral` and `Gemma`. I have traied Mixtral also but it dosent perform as good as `Llama3`."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"QmUBVEnvCDJv","executionInfo":{"status":"error","timestamp":1745167361366,"user_tz":-330,"elapsed":19,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}},"colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"d9e86142-c8a0-4afc-9bf7-c7b9c70e1d5e"},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-f97894e05705>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m \u001b[0;31m# Choose any! We auto support RoPE Scaling internally!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# Use 4bit quantization to reduce memory usage. Can be False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Fix Xformers performance issues since 0.0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n","fourbit_models = [\n","    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n","    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n","    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n","    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n","    \"unsloth/llama-3-70b-bnb-4bit\",\n","    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n","    \"unsloth/Phi-3-medium-4k-instruct\",\n","    \"unsloth/mistral-7b-bnb-4bit\",\n","    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n","] # More models at https://huggingface.co/unsloth\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"markdown","metadata":{"id":"SXd9bTZd1aaL"},"source":["We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bZsfBuZDeCL","executionInfo":{"status":"aborted","timestamp":1745167285342,"user_tz":-330,"elapsed":2,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"markdown","metadata":{"id":"ZaOf7N7twzbq"},"source":["Let's prepare dataset from the filtered Whatsapp Chat data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AhDjFQSRq6v","executionInfo":{"status":"aborted","timestamp":1745167285345,"user_tz":-330,"elapsed":159846,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["import pandas as pd\n","from datasets import Dataset, load_dataset\n","from unsloth.chat_templates import get_chat_template"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSS0Znn4wzbq","executionInfo":{"status":"aborted","timestamp":1745167285347,"user_tz":-330,"elapsed":159848,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template=\"llama-3\",  # Use the desired chat template\n","    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"}\n",")\n","\n","# Define the formatting function\n","def formatting_prompts_func(examples):\n","    convos = examples[\"conversations\"]\n","    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n","    return {\"text\": texts}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFP3QWaViVi8","executionInfo":{"status":"aborted","timestamp":1745167285348,"user_tz":-330,"elapsed":159848,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["df = pd.read_csv(\"all_chat_data.csv\")\n","conversations = []\n","for _, row in df.iterrows():\n","    try:\n","        conversation = [\n","            {'from': 'human', 'value': str(row['Prompt'])},\n","            {'from': 'assistant', 'value': str(row['Response'])}\n","        ]\n","        conversations.append(conversation)\n","    except:\n","        print(_ , row)\n","\n","\n","dataset = Dataset.from_dict({\"conversations\": conversations})\n","dataset = dataset.map(formatting_prompts_func, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"EdsJiFbRwzbq"},"source":["Let's see how the `Llama-3` format works by printing the 5th element"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuCMQuonRq0n","executionInfo":{"status":"aborted","timestamp":1745167285348,"user_tz":-330,"elapsed":159848,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["dataset[5][\"conversations\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJQ66lSywzbr","executionInfo":{"status":"aborted","timestamp":1745167285349,"user_tz":-330,"elapsed":159849,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["print(dataset[5][\"text\"])"]},{"cell_type":"markdown","metadata":{"id":"idAEIeSQ3xdS"},"source":["\n","### Train the model\n","Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer).\n","* I am doing `1 epochs` to speed things up, but you can set `num_train_epochs` to 2 or 3, Just experiment with it.\n","* If you have large dataset then just go for `1 full epoch`. Do not do more than 3 or 4 epoch if the `training loss` is not **decreasing**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95_Nn-89DhsL","executionInfo":{"status":"aborted","timestamp":1745167285350,"user_tz":-330,"elapsed":159850,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        num_train_epochs=1,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"markdown","metadata":{"id":"op6dTiGddipN"},"source":["#### Start Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqxqAZ7KJ4oL","executionInfo":{"status":"aborted","timestamp":1745167285383,"user_tz":-330,"elapsed":159882,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["trainer_stats = trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"ekOmTR1hSNcr"},"source":["<a name=\"Inference\"></a>\n","### Inference\n","Let's run the model! Since we're using `Llama-3`, use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR3gIAX-SM2q","executionInfo":{"status":"aborted","timestamp":1745167285385,"user_tz":-330,"elapsed":159884,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["from unsloth.chat_templates import get_chat_template\n","from transformers import TextStreamer\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n","    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",")\n","text_streamer = TextStreamer(tokenizer)\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"from\": \"human\", \"value\": \"Who are you?\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","output = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"]},{"cell_type":"markdown","metadata":{"id":"CrSvZObor0lY"},"source":[" You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"]},{"cell_type":"markdown","metadata":{"id":"uMuVrWbjAzhc"},"source":["<a name=\"Save\"></a>\n","### Saving finetuned model (LoRA adapter only)\n","\n","**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upcOlWe7A1vc","executionInfo":{"status":"aborted","timestamp":1745167285386,"user_tz":-330,"elapsed":159885,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["model.save_pretrained(\"lora_model\") # Local saving\n"]},{"cell_type":"markdown","metadata":{"id":"TCv4vXHd61i7"},"source":["### GGUF Conversion (For Ollama)\n","**To use our finetuned model in our PC/laptop we will use [Ollama](https://ollama.com/)**. To use this model with  `Ollama` We have to save the model in `GGUF` Format.\n","\n","Unsloth allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n","\n","Some supported quant methods (full list on Unsloth [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n","    * `q8_0` - Fast conversion. High resource use, but generally acceptable.\n","    * `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n","    * `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqfebeAdT073","executionInfo":{"status":"aborted","timestamp":1745167285387,"user_tz":-330,"elapsed":159881,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"outputs":[],"source":["# Save to 8bit Q8_0\n","if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n","if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")"]},{"cell_type":"markdown","metadata":{"id":"bDp0zNpwe6U_"},"source":["Once the fine-tuning is complete, the model `unsloth.Q8_0.gguf` is saved in the `models/` folder. You need to download this `GGUF` file and `Modelfile` too. You can directly download it by locating the file in the files section of Google Colab, or you can copy this file to your Google Drive to download it from there. If your internet connection is slow like mine, the Google Drive method is best because the file is large (approximately 8GB). Here are the steps for both methods."]},{"cell_type":"markdown","metadata":{"id":"E6GXCfCxwzbv"},"source":["### Direct Download via Colab\n","1. Click on the files section in Google Colab.\n","2. Locate the models folder. Then expand it by clicking on the arrow located to the left of the folder name.\n","3. Choose the file `unsloth.Q8_0.gguf` & `Modelfile`, then hover the mouse cursor over the filename.\n","4. Click on the three dots, then select Download.\n","\n","  <img src=https://github.com/Eviltr0N/Make-AI-Clone-of-Yourself/raw/main/img/file_download.png  width=\"400\" height=\"700\">"]},{"cell_type":"markdown","metadata":{"id":"CZAEERK4Tj1C"},"source":["### Using Finetuned Model With Ollama and Whatsapp\n","Now follow this guide on my [Github](https://github.com/Eviltr0N/Make-AI-Clone-of-Yourself?tab=readme-ov-file#loading-the-model-into-ollama) to chat with Your finetuned model.\n","[Here](https://github.com/Eviltr0N/Make-AI-Clone-of-Yourself?tab=readme-ov-file#loading-the-model-into-ollama)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"s8clu3XVrflN","executionInfo":{"status":"aborted","timestamp":1745167285397,"user_tz":-330,"elapsed":159889,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir /content/drive/MyDrive/finetuned_model\n","!cp /content/model/unsloth.Q8_0.gguf /content/drive/MyDrive/finetuned_model/\n"],"metadata":{"id":"G-H8hy4Vr7zL","executionInfo":{"status":"aborted","timestamp":1745167285399,"user_tz":-330,"elapsed":159890,"user":{"displayName":"pooja gupta","userId":"06700380135397116991"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1OGkiAZsYfShY0o8ZphCUuXkmb2Om422X","timestamp":1745135141202}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}